{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "144j4w73wvFt"
      },
      "outputs": [],
      "source": [
        "# Install library\n",
        "!pip install -U transformers\n",
        "!pip install accelerate\n",
        "# Download Dataset\n",
        "!wget https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip\n",
        "!wget https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip\n",
        "!unzip /content/Flickr8k_Dataset.zip\n",
        "!unzip /content/Flickr8k_text.zip\n",
        "\n",
        "!echo \"Downloaded Flickr8k dataset successfully.\"\n",
        "!mkdir saved_model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate"
      ],
      "metadata": {
        "id": "-6_dYtgm6mjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2jLbkeV8clS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97ff3e05-fd54-463e-8973-4142540166fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from transformers import AutoProcessor, AutoImageProcessor, ViTModel, CLIPVisionModel\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from huggingface_hub import login\n",
        "\n",
        "HUGGINGFACE_TOKEN = \"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\"\n",
        "login(HUGGINGFACE_TOKEN)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Architecture"
      ],
      "metadata": {
        "id": "dy_3zGvMl_Xk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyAdaptor(nn.Module) :\n",
        "  def __init__(self, num_vis_token_summary, vis_token_embedding_size, word_embedding_size, num_vocab) :\n",
        "    super(MyAdaptor, self).__init__()\n",
        "    self.num_vis_token_summary = num_vis_token_summary\n",
        "    self.vis_token_embedding_size = vis_token_embedding_size\n",
        "    self.word_embedding_size = word_embedding_size\n",
        "    self.num_vocab = num_vocab\n",
        "\n",
        "    self.adapter_embed = nn.Parameter(torch.randn(1,self.num_vis_token_summary, self.vis_token_embedding_size)*0.001)\n",
        "    self.adapter_MLP = nn.Sequential(\n",
        "        nn.Linear(self.vis_token_embedding_size, self.vis_token_embedding_size*4),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(self.vis_token_embedding_size*4, self.word_embedding_size)\n",
        "        # nn.Linear(1024, self.num_vocab)\n",
        "    )\n",
        "\n",
        "  def forward(self, img_output) :\n",
        "    temp_adap_embed = self.adapter_embed.to(img_output.device).to(img_output.dtype)\n",
        "    self.adapter_MLP.to(img_output.device)\n",
        "\n",
        "    attn_weight = temp_adap_embed @ img_output.permute(0,2,1)\n",
        "    attn_weight = torch.softmax(attn_weight,-1)\n",
        "    img_embed = attn_weight @ img_output\n",
        "\n",
        "    img_embed = self.adapter_MLP(img_embed)\n",
        "    # img_embed = torch.softmax(img_embed,-1) @ self.model_language.model.decoder.embed_tokens.weight\n",
        "\n",
        "    return img_embed"
      ],
      "metadata": {
        "id": "oAP9y9UmUvsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LearableToken(nn.Module) :\n",
        "  def __init__(self, num_token, word_embedding_size) :\n",
        "    super(LearableToken, self).__init__()\n",
        "    self.num_token = num_token\n",
        "    self.word_embedding_size = word_embedding_size\n",
        "    self.bias_token = nn.Parameter(torch.randn(self.num_token, self.word_embedding_size)*0.01)\n",
        "    self.instruct_bias_token = nn.Parameter(torch.randn(self.num_token*8, self.word_embedding_size)*0.01)\n"
      ],
      "metadata": {
        "id": "Ogrim6wWlFVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(nn.Module) :\n",
        "  def __init__(self) :\n",
        "    super(MyModel, self).__init__()\n",
        "    self.model_language = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-2b-it\", torch_dtype=torch.bfloat16)\n",
        "    # self.model_language = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\")\n",
        "    self.tokenizer_language = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\", padding_side= 'right')\n",
        "    self.image_processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\").image_processor\n",
        "    self.model_image = CLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "    print(self.model_language)\n",
        "    print(self.model_image)\n",
        "\n",
        "    self.word_embedding_size = 2048\n",
        "    self.num_vocab = 256000\n",
        "\n",
        "    self.trigger_str_img = \"<start_image>\"\n",
        "    self.num_vis_token_summary = 32\n",
        "    self.vis_token_embedding_size = 768\n",
        "    self.adaptorListCaption = MyAdaptor(self.num_vis_token_summary,self.vis_token_embedding_size,self.word_embedding_size,self.num_vocab )\n",
        "\n",
        "    self.dummy_img_token = (\" \".join([\"the\"]*self.num_vis_token_summary)).strip()\n",
        "    self.dummy_bias_token = (\" \".join([\"the\"]*self.num_bias_token)).strip()\n",
        "\n",
        "  def search_trigger_idx(self, text_token, trigger_str) :\n",
        "    all_token = text_token\n",
        "    all_string_now = \"\"\n",
        "    all_token_now = []\n",
        "    dummy_start_token = None\n",
        "    for token_idx in range(len(all_token)) :\n",
        "      token_now = int(all_token[token_idx].detach().cpu().numpy())\n",
        "      all_token_now.append(token_now)\n",
        "      token_as_string = self.tokenizer_language.batch_decode([all_token_now],skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
        "\n",
        "      if trigger_str in token_as_string :\n",
        "        dummy_start_token = token_idx + 1\n",
        "        break\n",
        "    return dummy_start_token\n",
        "\n",
        "  def preproc_image(self, image_local_link) :\n",
        "    image = Image.open(image_local_link)\n",
        "    inputs = self.image_processor(image, return_tensors=\"pt\")\n",
        "\n",
        "    return inputs\n",
        "\n",
        "\n",
        "  def get_image_embed(self, image_input) :\n",
        "    img_output = self.model_image(image_input)['last_hidden_state']\n",
        "    img_embed = self.adaptorListCaption.adaptor_embedding(img_output)\n",
        "\n",
        "    return img_embed\n",
        "\n",
        "  def get_bias_token_attn_mask(self,attention_mask_late,bias_token_now) :\n",
        "    new_attn_mask_now = attention_mask_late.to(float)\n",
        "    B, L = new_attn_mask_now.shape\n",
        "    new_attn_mask_now = new_attn_mask_now.reshape(B,L,1) @ new_attn_mask_now.reshape(B,1,L)\n",
        "    new_attn_mask_now = torch.tril(new_attn_mask_now)\n",
        "\n",
        "    new_attn_mask_now[:,1:1+len(bias_token_now),:] = 0 # B x L x L\n",
        "    new_attn_mask_now[:,1:1+len(bias_token_now),1:1+len(bias_token_now)] = 1\n",
        "\n",
        "    return new_attn_mask_now\n",
        "\n",
        "  def replace_embedding_hook(self, image_input) :\n",
        "    image_feature = self.get_image_embed(image_input)\n",
        "    assert len(image_feature) == 1\n",
        "\n",
        "    def now_hook(model, input, output) :\n",
        "      real_input = input[0]\n",
        "      batch_size, token_len = real_input.shape\n",
        "      if(token_len > 1) :\n",
        "        assert batch_size == 1\n",
        "        dummy_start_token = self.search_trigger_idx(real_input[0], self.trigger_str_img )\n",
        "\n",
        "        temp = image_feature[0]\n",
        "        output[:,dummy_start_token:dummy_start_token+self.num_vis_token_summary] = temp\n",
        "      return output\n",
        "    return now_hook\n",
        "\n",
        "\n",
        "\n",
        "  def split_and_replace(self, now_input_tokens, replacement_embed, start_loc) :\n",
        "    num_token = len(replacement_embed)\n",
        "\n",
        "    start_embed = now_input_tokens[0:start_loc]\n",
        "    end_embed = now_input_tokens[start_loc+num_token:]\n",
        "    replaced_embed = torch.cat((start_embed, replacement_embed.to(now_input_tokens.dtype), end_embed),0)\n",
        "\n",
        "    return replaced_embed\n",
        "\n",
        "  def forward_loss(self, image_input_raw, caption_output_raw) :\n",
        "    instruction_now =  self.dummy_bias_token + \"<start_of_turn>user\\n\"\n",
        "    instruction_now += f\"<start_image> {self.dummy_img_token}\\n<end_image>\\n\"\n",
        "    instruction_now += f\"Create a simple description of the image!\\n<end_of_turn>\\n<start_of_turn>model\\n\"\n",
        "\n",
        "    image_input = self.image_processor(image_input_raw, return_tensors=\"pt\")['pixel_values']\n",
        "    image_input = image_input.to(device)\n",
        "\n",
        "    caption_output = self.tokenizer_language(caption_output_raw,padding=True,return_tensors=\"pt\")\n",
        "    caption_output['input_ids'] = caption_output['input_ids'].to(device)\n",
        "    caption_output['attention_mask'] = caption_output['attention_mask'].to(device)\n",
        "\n",
        "    img_output = self.model_image(image_input)['last_hidden_state']\n",
        "    img_embed = self.adaptorListCaption.adaptor_embedding(img_output)\n",
        "\n",
        "    all_text_with_prompt = [instruction_now + temp_text for temp_text in self.tokenizer_language.batch_decode(caption_output['input_ids'], skip_special_tokens=True)]\n",
        "    all_tokens_with_prompt = self.tokenizer_language(all_text_with_prompt, padding=True, return_tensors=\"pt\")\n",
        "    all_tokens_with_prompt['input_ids'] = all_tokens_with_prompt['input_ids'].to(device).detach()\n",
        "    all_tokens_with_prompt['attention_mask'] = all_tokens_with_prompt['attention_mask'].to(device).detach()\n",
        "\n",
        "    all_token_prompt_embed = self.model_language.model.embed_tokens(all_tokens_with_prompt['input_ids'])\n",
        "    prompt_len = len(self.tokenizer_language([instruction_now])['input_ids'][0])\n",
        "    caption_label_now = all_tokens_with_prompt['input_ids'][:,prompt_len:]\n",
        "    caption_label_now = F.one_hot(caption_label_now,self.num_vocab)\n",
        "    attn_mask_now = all_tokens_with_prompt['attention_mask'][:,prompt_len:]\n",
        "\n",
        "    all_replaced_feature = []\n",
        "    for temp_idx in range(len(all_tokens_with_prompt['input_ids'])) :\n",
        "      tokens_text_now = all_tokens_with_prompt['input_ids'][temp_idx].detach().cpu()\n",
        "      dummy_location_caption = self.search_trigger_idx(tokens_text_now, self.trigger_str_img )\n",
        "\n",
        "      replaced_begin_task = self.split_and_replace(all_token_prompt_embed[temp_idx], self.learnableToken.bias_token ,1 )\n",
        "      image_replaced_prompt = self.split_and_replace(replaced_begin_task, img_embed[temp_idx], dummy_location_caption)\n",
        "\n",
        "      all_replaced_feature.append(image_replaced_prompt)\n",
        "\n",
        "\n",
        "    all_replaced_feature = torch.stack(all_replaced_feature)\n",
        "\n",
        "    new_attn_mask_now = self.get_bias_token_attn_mask(all_tokens_with_prompt['attention_mask'], self.learnableToken.bias_token)\n",
        "    logits_now = self.model_language(inputs_embeds =all_replaced_feature, attention_mask=new_attn_mask_now)\n",
        "\n",
        "    logits_now = logits_now['logits']\n",
        "    caption_prediction_now = logits_now[:,prompt_len-1:-1]\n",
        "    caption_prediction_now = torch.softmax(caption_prediction_now,-1)\n",
        "\n",
        "    loss_lm = -torch.sum(caption_label_now*torch.log(caption_prediction_now),-1)\n",
        "    loss_lm = torch.sum(loss_lm*attn_mask_now,-1)/torch.sum(attn_mask_now,-1)\n",
        "    loss_lm = torch.mean(loss_lm)\n",
        "\n",
        "    return loss_lm, caption_prediction_now\n",
        "\n",
        "  def generate_aswer_image(self, input_with_dummy_prompt, pil_image = None, max_new_tokens = 32, do_sample=True, top_k=50, top_p=0.95, temperature =1 ) :\n",
        "\n",
        "    dummy_input = self.tokenizer_language(input_with_dummy_prompt,padding=True,return_tensors=\"pt\")\n",
        "    dummy_input['input_ids'] = dummy_input['input_ids'].to(device)\n",
        "    dummy_input['attention_mask'] = dummy_input['attention_mask'].to(device)\n",
        "    assert len(dummy_input['input_ids']) == 1\n",
        "\n",
        "    handler_image = None\n",
        "\n",
        "    contains_image = False\n",
        "    if self.trigger_str_img in input_with_dummy_prompt :\n",
        "      image_input = self.image_processor(pil_image, return_tensors=\"pt\")['pixel_values'].to(device)\n",
        "\n",
        "      if len(image_input) == 1 :\n",
        "        hook_now_image = self.replace_embedding_hook(image_input)\n",
        "        contains_image = True\n",
        "        handler_image = self.model_language.model.embed_tokens.register_forward_hook(hook_now_image)\n",
        "      elif len(image_input) > 1  :\n",
        "        hook_now_image = self.replace_embedding_hook_multiple(image_input)\n",
        "        contains_image = True\n",
        "        handler_image = self.model_language.model.embed_tokens.register_forward_hook(hook_now_image)\n",
        "\n",
        "\n",
        "    output_now = self.model_language.generate(**dummy_input,\n",
        "                                              max_new_tokens = max_new_tokens,\n",
        "                                              do_sample=do_sample,\n",
        "                                              temperature=temperature,\n",
        "                                              top_k=top_k,\n",
        "                                              top_p=top_p,\n",
        "                                              )\n",
        "    output_string = self.tokenizer_language.batch_decode(output_now, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
        "\n",
        "    if contains_image :\n",
        "      handler_image.remove()\n",
        "\n",
        "    return output_string\n",
        "\n"
      ],
      "metadata": {
        "id": "zDjJ7IKJVlLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter"
      ],
      "metadata": {
        "id": "5yMfwZI3mD2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CAPTION_PATH = \"/content/Flickr8k.token.txt\"\n",
        "IMAGES_FILE_PATH = \"/content/Flicker8k_Dataset\"\n",
        "SAVED_PATH = \"/content/saved_model/adaptor_caption.pt\"\n",
        "\n",
        "BATCH_SIZE = 4\n",
        "NUM_ITERATION = 2000\n",
        "SAVE_EVERY = 500\n",
        "LEARNING_RATE = 1e-4\n",
        "TRAIN_DATA_NUM = 7500\n",
        "\n",
        "device = 'cpu'\n",
        "if torch.cuda.is_available() :\n",
        "  device = 'cuda'"
      ],
      "metadata": {
        "id": "0-TcUiYMl7LP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Util Function"
      ],
      "metadata": {
        "id": "w5cCXJvGnKek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_model_nan(model) :\n",
        "  num_nan = 0\n",
        "  for param in model.parameters() :\n",
        "    num_nan += torch.sum(torch.isnan(param))\n",
        "  return num_nan > 0"
      ],
      "metadata": {
        "id": "Q49xyNY9nLrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getLabelDictionary(file_path) :\n",
        "  file_now = open(file_path)\n",
        "  all_string = file_now.read()\n",
        "  all_string = all_string.split('\\n')\n",
        "  label_dictionary = {}\n",
        "  for line_now in all_string :\n",
        "    splitted_line = line_now.split('\\t')\n",
        "    if len(splitted_line) > 1 :\n",
        "      file_name_now = splitted_line[0].split('#')[0]\n",
        "      number_now = splitted_line[0].split('#')[1]\n",
        "      label_now = splitted_line[1]\n",
        "\n",
        "      if file_name_now in label_dictionary.keys() :\n",
        "        label_dictionary[file_name_now].append(label_now)\n",
        "      else :\n",
        "        label_dictionary.update({\n",
        "            file_name_now:[label_now]\n",
        "        })\n",
        "\n",
        "  return label_dictionary"
      ],
      "metadata": {
        "id": "HDGy92AnnQik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_model_param(model_now) :\n",
        "  counter = 0\n",
        "  for param in model_now.parameters() :\n",
        "    counter = counter + torch.sum(torch.ones_like(param))\n",
        "  return counter"
      ],
      "metadata": {
        "id": "Y1t-qJVdnV7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_data_caption(file_list_now, caption_dict_now, n) :\n",
        "  # base_path = \"/content/Flicker8k_Dataset\"\n",
        "  base_path = IMAGES_FILE_PATH\n",
        "  rand_idx = np.random.randint(0,len(file_list_now), n)\n",
        "\n",
        "  all_image = []\n",
        "  all_text = []\n",
        "  for idx_now in rand_idx :\n",
        "    file_now = base_path + \"/\" + file_list_now[idx_now]\n",
        "    image_now = Image.open(file_now)\n",
        "    all_image.append(image_now)\n",
        "\n",
        "    text_list_now = caption_dict_now[file_list_now[idx_now]]\n",
        "    selected_text_now_idx = np.random.randint(0,len(text_list_now))\n",
        "    all_text.append(text_list_now[selected_text_now_idx])\n",
        "\n",
        "  return all_image, all_text"
      ],
      "metadata": {
        "id": "SrJ1BBE2nYMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialization"
      ],
      "metadata": {
        "id": "cr2uJ5sd4K9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_dictionary = getLabelDictionary(CAPTION_PATH)\n",
        "all_file = os.listdir(IMAGES_FILE_PATH)"
      ],
      "metadata": {
        "id": "7CWxhtqD3IgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel()\n",
        "model = model.to(device)\n",
        "model = model.to(torch.bfloat16)"
      ],
      "metadata": {
        "id": "WQjnonoW4S0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if load model\n",
        "if os.path.exists(SAVED_PATH) :\n",
        "  model.adaptorListCaption.load_state_dict(torch.load(SAVED_PATH))"
      ],
      "metadata": {
        "id": "fdAhcitm5WVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model.parameters() :\n",
        "  param.requires_grad = True\n",
        "for param in model.model_language.parameters() :\n",
        "  param.requires_grad = False\n",
        "for param in model.model_image.parameters() :\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "3_xvEL5o5XHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optim = torch.optim.Adam(model.parameters(),LEARNING_RATE)"
      ],
      "metadata": {
        "id": "KGRlhJmm5aZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "ofPSgNuynaIh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "for itr in range(NUM_ITERATION) :\n",
        "  print(\"ITERATION:\", itr, \"/\", NUM_ITERATION)\n",
        "\n",
        "  rand_image, rand_targets = sample_data_caption(all_file, label_dictionary, BATCH_SIZE)\n",
        "  loss = model.forward_loss(rand_image, rand_targets)\n",
        "  optim.zero_grad()\n",
        "  loss.backward()\n",
        "  optim.step()\n"
      ],
      "metadata": {
        "id": "L_zA9eeB5UzU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}